{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python fanout with error prone tasks.\n",
    "Supposedly you have tasks to be done parallely but some of them are prone to failure. Let's define such a piece of work using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "CustomError",
     "evalue": "I would if I could",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCustomError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9c81dc54b73f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msketchy_work\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'some_value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-9c81dc54b73f>\u001b[0m in \u001b[0;36msketchy_work\u001b[0;34m(prob_of_fail, token)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mprob_of_fail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrandom_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCustomError\u001b[0m: I would if I could"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class CustomError(Exception):\n",
    "    pass\n",
    "\n",
    "def random_error(): # return random error from three pre-defined ones.\n",
    "    return np.random.choice([CustomError(\"I hate mondays\"), \n",
    "                             CustomError(\"I am lazy\"), \n",
    "                             CustomError(\"I would if I could\")])\n",
    "\n",
    "def sketchy_work(prob_of_fail, token): # return token or raise error with probability prob_of_fail\n",
    "    time.sleep(1)\n",
    "    if np.random.uniform() <= prob_of_fail:\n",
    "        raise random_error()\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "sketchy_work(0.9, 'some_value') # 90% of time it will raise one of custom errors defined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a simple wrapper to put our `sketchy_work` in a try-catch statement, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am lazy\n"
     ]
    }
   ],
   "source": [
    "def sketchy_work_wrapper(prob_of_fail, token):\n",
    "    try:\n",
    "        sketchy_work(prob_of_fail, token)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "sketchy_work_wrapper(0.9, 'some_work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when running a fanout using multiprocessing returning error from process is not that easy. They do not appear in any predictable order, and sometimes not every error is populated for the user to view it. So dealing with it, I found it difficult to report all the issues. Let us modify this code a bit. We will add shared variables which for now will not make that much sense but stay with me for a bit longer. It will get clear soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketchy_work_wrapper(prob_of_fail, token,\n",
    "                         shared_success_counter,\n",
    "                         shared_skips_counter,\n",
    "                         shared_skip_reasons,\n",
    "                         shared_lock):\n",
    "\n",
    "    try:\n",
    "        sketchy_work(prob_of_fail, token)\n",
    "        with shared_lock:\n",
    "            shared_success_counter.value += 1\n",
    "    except Exception as e:\n",
    "        with shared_lock:\n",
    "            shared_skip_reasons[token] = str(e)\n",
    "            shared_skips_counter.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified code uses shared variables which are a special variables provided by multiprocessing library. These values are shared between processes, but to prevent data raids, we will also include a `Lock()` object, which ensures our variable is not accessed when its value is already in modification by other process. A one of famous actions that we want to prevent is for instance one process taking counter value `n` and before updating it to `n+1` other process takes value `n` and after first returns `n+1` second process also returns `n+1` which is wrong because our counter should be `n+2` at this point. This is what I call \"accessing an obsolete value\". Putting an action of modifing variable in a `with Lock()` statement, it ensures this scenario will not happen. For dictionary variable, every process is only modyfing its own entry, hence this will not happen, but we still can use lock as it will not harm our code. Now let's see how we introduce all those variables, and how we will conduct fanout with multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_wrapper(func):\n",
    "\n",
    "    with mp.Manager() as mgr:\n",
    "        shared_lock = mgr.Lock()\n",
    "        shared_success_counter = mgr.Value('successes', 0)\n",
    "        shared_skips_counter = mgr.Value('skips', 0)\n",
    "        shared_skip_reasons = mgr.dict()\n",
    "\n",
    "        p = mp.Pool(mp.cpu_count())\n",
    "        p.starmap(func, [(0.6, i,\n",
    "                          shared_success_counter,\n",
    "                          shared_skips_counter,\n",
    "                          shared_skip_reasons,\n",
    "                          shared_lock) for i in range(20)])\n",
    "        p.close()\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will open shared context manager and introduce our variables. Then we will introduce a `Pool` with number of processes that our machine can handle using `mp.cpu_count()` method. Then we will use starmap to map executions to processes. In my case I will prepare 20 different executions (with probability of failure of 60%) and map them to my 8 cpu's. Since every execution is sleeping for 1 second, I should expect for this to run about ceil(20/8) = 3 seconds. But before that we still would like to use those shared variables somehow for instance preparing some usefull information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_wrapper(func):\n",
    "\n",
    "    with mp.Manager() as mgr:\n",
    "        shared_lock = mgr.Lock()\n",
    "        shared_success_counter = mgr.Value('successes', 0)\n",
    "        shared_skips_counter = mgr.Value('skips', 0)\n",
    "        shared_skip_reasons = mgr.dict()\n",
    "\n",
    "        p = mp.Pool(AVAILABLE_COMPUTING_POWER)\n",
    "        p.starmap(func, [(0.6, i,\n",
    "                          shared_success_counter,\n",
    "                          shared_skips_counter,\n",
    "                          shared_skip_reasons,\n",
    "                          shared_lock) for i in range(20)])\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "        # NEW\n",
    "        wrapper_successes = shared_success_counter.value\n",
    "        wrapper_skips = shared_skips_counter.value\n",
    "        wrapper_reasons = dict(shared_skip_reasons)\n",
    "\n",
    "    most_common_skips = Counter(wrapper_reasons.values()).most_common()\n",
    "    message = '\\n'\n",
    "    for s in most_common_skips:\n",
    "        message += f\"occurred: {s[1]} | reason: {s[0]}\\n\"\n",
    "\n",
    "    print(f\"SUCCEEDED: {wrapper_successes}, SKIPPED: {wrapper_skips}\")\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying and printing those variables should benefit our understanding of how executions went. Now it is time to execute all this. Full example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "AVAILABLE_COMPUTING_POWER = mp.cpu_count()\n",
    "\n",
    "class CustomError(Exception):\n",
    "    pass\n",
    "\n",
    "def random_error():\n",
    "    return np.random.choice([CustomError(\"I hate mondays\"), \n",
    "                             CustomError(\"I am lazy\"), \n",
    "                             CustomError(\"I would if I could\")])\n",
    "\n",
    "def sketchy_work(prob_of_fail, token):\n",
    "    time.sleep(1)\n",
    "    if np.random.uniform() <= prob_of_fail:\n",
    "        raise random_error()\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "def sketchy_work_wrapper(prob_of_fail, token,\n",
    "                         shared_success_counter,\n",
    "                         shared_skips_counter,\n",
    "                         shared_skip_reasons,\n",
    "                         shared_lock):\n",
    "\n",
    "    try:\n",
    "        sketchy_work(prob_of_fail, token)\n",
    "        with shared_lock:\n",
    "            shared_success_counter.value += 1\n",
    "    except Exception as e:\n",
    "        with shared_lock:\n",
    "            shared_skip_reasons[token] = str(e)\n",
    "            shared_skips_counter.value += 1\n",
    "\n",
    "\n",
    "def parallel_wrapper(func):\n",
    "\n",
    "    with mp.Manager() as mgr:\n",
    "        shared_lock = mgr.Lock()\n",
    "        shared_success_counter = mgr.Value('successes', 0)\n",
    "        shared_skips_counter = mgr.Value('skips', 0)\n",
    "        shared_skip_reasons = mgr.dict()\n",
    "\n",
    "        p = mp.Pool(AVAILABLE_COMPUTING_POWER)\n",
    "        p.starmap(func, [(0.6, i,\n",
    "                          shared_success_counter,\n",
    "                          shared_skips_counter,\n",
    "                          shared_skip_reasons,\n",
    "                          shared_lock) for i in range(20)])\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "        wrapper_successes = shared_success_counter.value\n",
    "        wrapper_skips = shared_skips_counter.value\n",
    "        wrapper_reasons = dict(shared_skip_reasons)\n",
    "\n",
    "    most_common_skips = Counter(wrapper_reasons.values()).most_common()\n",
    "    message = '\\n'\n",
    "    for s in most_common_skips:\n",
    "        message += f\"occurred: {s[1]} | reason: {s[0]}\\n\"\n",
    "\n",
    "    print(f\"SUCCEEDED: {wrapper_successes}, SKIPPED: {wrapper_skips}\")\n",
    "    print(message)\n",
    "\n",
    "# NEW\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    parallel_wrapper(sketchy_work_wrapper)\n",
    "    print(f\"RAN ON {AVAILABLE_COMPUTING_POWER} PROCESSES, IN {time.time()-start:.2f}[s]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output](mp_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
